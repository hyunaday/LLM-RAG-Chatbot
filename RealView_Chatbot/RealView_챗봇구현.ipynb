{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBmVyurMmZYS",
        "outputId": "a570ff2e-6111-4e37-c226-4640bfaa043f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m326.6/326.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit langchain langchain_experimental langchain-openai langchain-community langchain-text-splitters chromadb python-dotenv pyngrok pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”‘ ngrok ì¸ì¦ í† í°ì„ ì—¬ê¸°ì— ë¶™ì—¬ë„£ê³  ì‹¤í–‰í•´ì£¼ì„¸ìš”!\n",
        "# \"YOUR_AUTHTOKEN_HERE\" ë¶€ë¶„ì„ í˜„ì•„ì˜ ì‹¤ì œ ngrok ì¸ì¦ í† í°ìœ¼ë¡œ ë°”ê¿”ì£¼ì„¸ìš”!\n",
        "!ngrok config add-authtoken \"YOUR_AUTHTOKEN_HERE\"\n",
        "print(\"âœ… ngrok ì¸ì¦ í† í° ë“±ë¡ ì™„ë£Œ!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlWfCxczmfMF",
        "outputId": "ac530870-cc4f-440f-93fb-9dadb5df4ee3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "âœ… ngrok ì¸ì¦ í† í° ë“±ë¡ ì™„ë£Œ!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# ì§ì ‘ í™˜ê²½ë³€ìˆ˜ ë“±ë¡ (ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
        "\n",
        "# í™•ì¸\n",
        "print(\"âœ… OPENAI_API_KEY ë“±ë¡ ì™„ë£Œ:\", os.environ.get(\"OPENAI_API_KEY\")[:10] + \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLwaufh6mrec",
        "outputId": "d48e6813-b2d9-4cb1-b50e-b0ec1ceeb646"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… OPENAI_API_KEY ë“±ë¡ ì™„ë£Œ: YOUR_OPENA...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os, uuid\n",
        "import streamlit as st\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_community.document_loaders import TextLoader, PyPDFLoader, WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from io import BytesIO\n",
        "import tempfile\n",
        "import asyncio\n",
        "import requests\n",
        "\n",
        "# --- Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸° ì„¤ì • ---\n",
        "st.set_page_config(page_title=\"RealView ì±—ë´‡ (AI ê¸°ë°˜ ì§€ì—­ ë¶„ì„)\", layout=\"wide\")\n",
        "st.title(\"ğŸ¡ RealView ì±—ë´‡: AI ê¸°ë°˜ ì§€ì—­ ë¶„ì„ ì±—ë´‡\")\n",
        "st.markdown(\"AI ê¸°ë°˜ ì§€ì—­ ì‹œì„¸ ë° ë¶„ìœ„ê¸° ë¶„ì„ ì±—ë´‡ 'RealView'ì…ë‹ˆë‹¤. ê¶ê¸ˆí•œ ì§€ì—­ ë˜ëŠ” ê¸°íƒ€ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ëŒ€í•´ ë¬¸ì˜í•´ ì£¼ì„¸ìš”! âœ¨\")\n",
        "\n",
        "# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”: ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒíƒœ ìœ ì§€ë¥¼ ìœ„í•¨ ---\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state[\"messages\"] = []\n",
        "if \"vectorstore\" not in st.session_state:\n",
        "    st.session_state[\"vectorstore\"] = None\n",
        "if \"web_loaded_urls\" not in st.session_state:\n",
        "    st.session_state[\"web_loaded_urls\"] = []\n",
        "if \"initial_learning_done\" not in st.session_state:\n",
        "    st.session_state[\"initial_learning_done\"] = False\n",
        "\n",
        "# --- LLM(Large Language Model) ì„¤ì • ---\n",
        "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0.7)\n",
        "\n",
        "# --- ì±—ë´‡ì˜ í˜ë¥´ì†Œë‚˜ ë° ì‘ë‹µ ë¡œì§ì„ ì •ì˜í•˜ëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ëŒ€í™” ì´ë ¥ í¬í•¨) ---\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"\"\"ë‹¹ì‹ ì€ 'RealView'ë¼ëŠ” ì´ë¦„ì˜ ì¹œì ˆí•˜ê³  ì‚¬ë ¤ ê¹Šì€ AI ë¶€ë™ì‚° ë¹„ì„œì´ì ì§€ì‹ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
        "ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê³ , ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ í˜„ì¬ ì§ˆë¬¸ì˜ ë§¥ë½ì„ ì—°ê²°í•˜ì—¬ ë³´ë‹¤ ìì—°ìŠ¤ëŸ½ê³  ìœ ì—°í•˜ê²Œ ì†Œí†µí•´ ì£¼ì‹­ì‹œì˜¤.\n",
        "\n",
        "ì‚¬ìš©ìê°€ íŠ¹ì • ì§€ì—­ì˜ ë¶€ë™ì‚° ì‹œì„¸, ê±°ë˜ ë™í–¥, ìƒí™œ ë¶„ìœ„ê¸°, í•™êµ°, í¸ì˜ì‹œì„¤ ë“± ë¶€ë™ì‚° ê´€ë ¨ ì§ˆë¬¸ì€ ë¬¼ë¡ ,\n",
        "ê·¸ ì™¸ ì–´ë–¤ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì„ í•˜ë”ë¼ë„ ì‚¬ìš©ìì˜ ê¶ê¸ˆì¦ì— ê¹Šì´ ê³µê°í•˜ë©°, ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ ì–´ì¡°ë¡œ ë‹µë³€í•´ ì£¼ì‹­ì‹œì˜¤.\n",
        "ë‹¨ìˆœíˆ ë‹µë³€ë§Œ ë‚˜ì—´í•˜ëŠ” ê²ƒì„ ë„˜ì–´, ì‚¬ìš©ìê°€ ì´ì „ì— ì–¸ê¸‰í•œ ë‚´ìš©ê³¼ ì—°ê´€ ì§€ì–´ ë‹µë³€í•˜ê±°ë‚˜,\n",
        "ë‹¤ìŒ ëŒ€í™”ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§ˆ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì´ë‚˜ ê´€ë ¨ ì •ë³´ ì œì•ˆì„ í•œë‘ ê°€ì§€ ì¶”ê°€í•˜ì—¬ ëŒ€í™”ì˜ íë¦„ì„ ì£¼ë„í•˜ì‹­ì‹œì˜¤.\n",
        "ë™ì¼í•˜ê±°ë‚˜ ìœ ì‚¬í•œ ì •ë³´ì˜ ë°˜ë³µì„ í”¼í•˜ê³ , í•­ìƒ ìƒˆë¡œìš´ ê´€ì ì´ë‚˜ ì¶”ê°€ì ì¸ í†µì°°ë ¥ì„ ì œê³µí•˜ë„ë¡ ë…¸ë ¥í•˜ì‹­ì‹œì˜¤.\n",
        "\n",
        "ì œê³µëœ 'ì°¸ê³  ë¬¸ì„œ' ë‚´ìš©ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ìœ¼ë¯€ë¡œ, ë‹µë³€ ì‹œ ì´ ë¬¸ì„œë¥¼ ìµœìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì‹­ì‹œì˜¤.\n",
        "ë§Œì•½ 'ì°¸ê³  ë¬¸ì„œ'ì— ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ì¶©ë¶„í•œ ì •ë³´ê°€ ì—†ê±°ë‚˜, ì§ˆë¬¸ ë‚´ìš©ì´ 'ì°¸ê³  ë¬¸ì„œ'ì™€ ì „í˜€ ê´€ë ¨ì´ ì—†ë‹¤ë©´,\n",
        "ë‹¹ì‹ ì˜ í­ë„“ì€ ì¼ë°˜ ì§€ì‹ê³¼ ìƒì‹ì„ í™œìš©í•˜ì—¬ ìµœëŒ€í•œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ ì£¼ì‹­ì‹œì˜¤.\n",
        "ë‹¨ìˆœíˆ \"ì°¸ê³  ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤\"ë¼ê³  ëŒ€ë‹µí•˜ê¸°ë³´ë‹¤ëŠ”, í•­ìƒ ìµœì„ ì„ ë‹¤í•´ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì°¸ê³  ë¬¸ì„œ: {context}\"\"\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"{q}\")\n",
        "])\n",
        "\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# --- ë¬¸ì„œ ë¡œë”©, ë¶„í• (chunking) ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” í•¨ìˆ˜ ---\n",
        "@st.cache_resource\n",
        "def create_vectorstore(documents):\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "    chunks = splitter.split_documents(documents)\n",
        "    emb = OpenAIEmbeddings(model='text-embedding-3-small')\n",
        "    db = Chroma.from_documents(chunks, emb)\n",
        "    return db\n",
        "\n",
        "# URLì˜ í™œì„±í™” ìƒíƒœë¥¼ í™•ì¸í•˜ëŠ” í•¨ìˆ˜ (ìºì‹± ì ìš©)\n",
        "@st.cache_data(ttl=3600)\n",
        "def check_url_active(url):\n",
        "    try:\n",
        "        response = requests.head(url, timeout=5, allow_redirects=True)\n",
        "        return response.status_code == 200\n",
        "    except requests.exceptions.RequestException:\n",
        "        return False\n",
        "\n",
        "# ëª¨ë“  í•™ìŠµ ë¬¸ì„œë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
        "all_documents = []\n",
        "\n",
        "# --- ê¸°ë³¸ ì§€ì‹ ë¬¸ì„œ (realview_data.md) ë¡œë”© ---\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "realview_doc_path = \"data/realview_data.md\"\n",
        "\n",
        "if not os.path.exists(realview_doc_path):\n",
        "    with open(realview_doc_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\n",
        "            \"## ê°•ë‚¨êµ¬ ì—­ì‚¼ë™ ì§€ì—­ ê°œìš”\\n\"\n",
        "            \"- ì—­ì‚¼ë™ì€ ì„œìš¸ ê°•ë‚¨êµ¬ì— ìœ„ì¹˜í•˜ë©°, í…Œí—¤ë€ë¡œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ìƒì—…, ì—…ë¬´ ì§€êµ¬ê°€ ë°œë‹¬í•´ ìˆìŠµë‹ˆë‹¤.\\n\"\n",
        "            \"- ë‹¤ì–‘í•œ ì—°ë ¹ëŒ€ì˜ ì¸êµ¬ê°€ ê±°ì£¼í•˜ë©°, ì§ì£¼ê·¼ì ‘ì„ ì„ í˜¸í•˜ëŠ” ì§ì¥ì¸ë“¤ì´ ë§ìŠµë‹ˆë‹¤.\\n\"\n",
        "            \"- êµìœ¡ í™˜ê²½ìœ¼ë¡œëŠ” ì—­ì‚¼ì´ˆë“±í•™êµ, ì§„ì„ ì—¬ìì¤‘í•™êµ/ê³ ë“±í•™êµ ë“±ì´ ìˆìœ¼ë©°, ëŒ€ì¹˜ë™ í•™ì›ê°€ì™€ ì¸ì ‘í•˜ì—¬ êµìœ¡ì—´ì´ ë†’ì€ í¸ì…ë‹ˆë‹¤.\\n\\n\"\n",
        "            \"## ê°•ë‚¨êµ¬ ì²­ë‹´ë™ ì§€ì—­ ê°œìš”\\n\"\n",
        "            \"- ì²­ë‹´ë™ì€ ê³ ê¸‰ ì£¼ê±° ë° ìƒì—… ì§€ì—­ìœ¼ë¡œ, ëª…í’ˆ ê±°ë¦¬ì™€ ê°¤ëŸ¬ë¦¬ ë“±ì´ ë°€ì§‘í•´ ìˆìŠµë‹ˆë‹¤.\\n\"\n",
        "            \"- ì¡°ìš©í•˜ê³  í”„ë¼ì´ë¹—í•œ ì£¼ê±° í™˜ê²½ì„ ì„ í˜¸í•˜ëŠ” ì´ë“¤ì—ê²Œ ì¸ê¸°ê°€ ë§ìŠµë‹ˆë‹¤.\\n\"\n",
        "            \"- ì²­ë‹´ê³ ë“±í•™êµ ë“± ëª…ë¬¸ í•™êµ°ì„ ê°–ì¶”ê³  ìˆìœ¼ë©°, í•œê°•ë³€ ì ‘ê·¼ì„±ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤.\\n\"\n",
        "            \"## ì„œìš¸ì‹œ ë¶€ë™ì‚° ì‹œì¥ ë™í–¥ (2025ë…„ ê¸°ì¤€ ì˜ˆì¸¡)\\n\"\n",
        "            \"- 2025ë…„ ë¶€ë™ì‚° ì‹œì¥ì€ ê¸°ì¤€ê¸ˆë¦¬ ì¸í•˜ ê°€ëŠ¥ì„±ê³¼ ì •ë¶€ì˜ ì™„í™” ì •ì±… ê¸°ì¡°ì— ë”°ë¼ ì ì§„ì ì¸ íšŒë³µì„¸ë¥¼ ë³´ì¼ ê²ƒìœ¼ë¡œ ì „ë§ë©ë‹ˆë‹¤.\\n\"\n",
        "            \"- íŠ¹íˆ ê°•ë‚¨ê¶Œì€ ê²¬ê³ í•œ ìˆ˜ìš”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒìŠ¹ ì—¬ë ¥ì´ ì¡´ì¬í•˜ë©°, ì¬ê±´ì¶• ë° ì¬ê°œë°œ ì´ìŠˆê°€ ì‹œì¥ì— ê¸ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\"\n",
        "            \"- ì‹¤ìˆ˜ìš”ì™€ íˆ¬ì ìˆ˜ìš”ì˜ ê· í˜•ì´ ì¤‘ìš”í•˜ë©°, íŠ¹ì • ì§€ì—­ì€ ì—¬ì „íˆ ë†’ì€ ê´€ì‹¬ì„ ë°›ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.\\n\\n\"\n",
        "            \"## ëŒ€í•œë¯¼êµ­ ë¶€ë™ì‚° íˆ¬ì ì›ì¹™\\n\"\n",
        "            \"- ë¶€ë™ì‚° íˆ¬ìëŠ” ì¥ê¸°ì ì¸ ê´€ì ì—ì„œ ì ‘ê·¼í•´ì•¼ í•©ë‹ˆë‹¤.\\n\"\n",
        "            \"- ì…ì§€ ë¶„ì„, ìˆ˜ìš”-ê³µê¸‰ ê· í˜•, ì •ì±… ë³€í™”, ê¸ˆë¦¬ ë™í–¥ ë“± ë‹¤ì–‘í•œ ìš”ì†Œë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\\n\"\n",
        "            \"- ë³¸ì¸ì˜ ìì‚° ìƒí™©ê³¼ ëª©í‘œì— ë§ëŠ” íˆ¬ì ì „ëµì„ ìˆ˜ë¦½í•˜ëŠ” ê²ƒì´ ì„±ê³µì ì¸ íˆ¬ìì˜ í•µì‹¬ì…ë‹ˆë‹¤.\\n\\n\"\n",
        "            \"## ì£¼ìš” ì§€ì—­ë³„ íŠ¹ì§•\\n\"\n",
        "            \"- ì†¡íŒŒêµ¬: ì ì‹¤ì„ ì¤‘ì‹¬ìœ¼ë¡œ ëŒ€ê·œëª¨ ì£¼ê±° ë‹¨ì§€ì™€ ìƒì—… ì‹œì„¤ì´ ë°œë‹¬. ì˜¬ë¦¼í”½ê³µì› ë“± ë…¹ì§€ ê³µê°„ í’ë¶€.\\n\"\n",
        "            \"- ìš©ì‚°êµ¬: í•œê°•ê³¼ ë‚¨ì‚°ì„ ë‚€ ê³ ê¸‰ ì£¼ê±° ì§€ì—­. ìš©ì‚° êµ­ì œì—…ë¬´ì§€êµ¬ ê°œë°œ ê³„íš ë“±ìœ¼ë¡œ ë¯¸ë˜ ê°€ì¹˜ ë†’ìŒ.\\n\"\n",
        "            \"- ì„±ë™êµ¬: ì„±ìˆ˜ë™ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì Šì€ ì¸µ ìœ ì… ì¦ê°€. íŠ¸ë Œë””í•œ ìƒì—… ì‹œì„¤ê³¼ ì§€ì‹ì‚°ì—…ì„¼í„° í™œì„±í™”.\"\n",
        "        )\n",
        "\n",
        "loader_text = TextLoader(realview_doc_path, encoding='utf-8')\n",
        "docs_from_text = loader_text.load()\n",
        "for doc in docs_from_text:\n",
        "    doc.metadata['source'] = os.path.basename(realview_doc_path)\n",
        "all_documents.extend(docs_from_text)\n",
        "\n",
        "\n",
        "# --- ìë™ ì›¹ í•™ìŠµ URL ëª©ë¡ ---\n",
        "predefined_web_urls = [\n",
        "    (\"ë„¤ì´ë²„ ë¶€ë™ì‚° ë¸”ë¡œê·¸ - ì£¼íƒ ë§¤ë§¤ì‹œì¥ íë¦„\", \"https://blog.naver.com/land_admin/223240465227\"),\n",
        "    (\"ë§¤ì¼ê²½ì œ ë¶€ë™ì‚° - ì„œìš¸ ì•„íŒŒíŠ¸ ë§¤ë§¤ ë™í–¥ ê¸°ì‚¬\", \"https://www.mk.co.kr/news/realestate/10862086\"),\n",
        "    (\"í•œêµ­ê²½ì œ ë¶€ë™ì‚° - ê¹€ìœ ë¯¸ì˜ ë˜‘ë˜‘í•œ ë¶€ë™ì‚° (íˆ¬ìë²•)\", \"https://www.hankyung.com/realestate/article/202305228189e\"),\n",
        "    (\"ë ˜êµ° ë¸”ë¡œê·¸ - ì•„íŒŒíŠ¸ íˆ¬ì í•µì‹¬ ì›ì¹™\", \"https://remgun.com/5176\"),\n",
        "    (\"KB ë¶€ë™ì‚° - ì£¼ê°„ ì£¼íƒì‹œì¥ ë™í–¥ ë¦¬í¬íŠ¸ (ìƒ˜í”Œ)\", \"https://kbland.kbstar.com/starlib/upload/KBdata20241029_112702.pdf\"),\n",
        "    (\"í•œêµ­ë¶€ë™ì‚°ì› - ë¶€ë™ì‚°ì‹œì¥ ë™í–¥ ì›”ê°„ë³´ê³ ì„œ (2024ë…„ 10ì›”)\", \"https://www.reb.or.kr/reb/statistics/trend/monthly_reports.jsp\"),\n",
        "    (\"êµ­í† êµí†µë¶€ - ë¶€ë™ì‚° ë³´ë„ìë£Œ (ìµœì‹  ì •ì±…)\", \"https://www.molit.go.kr/USR/policyTarget/m_24066/dtl.jsp\"),\n",
        "    (\"ì§ë°© ë¹…ë°ì´í„° ë© - 2024ë…„ ì£¼íƒì‹œì¥ ì „ë§\", \"https://news.zigbang.com/zigbanglab/report-4537\"),\n",
        "    (\"ê²½ì œ ë‰´ìŠ¤ - í•œêµ­ê²½ì œì—°êµ¬ì› ë¶€ë™ì‚°ì‹œì¥ ë¶„ì„\", \"https://www.keri.org/research/view.asp?p_no=0&idx=19565&cls_code=&st_str=&s_year=&e_year=&menu=&tab=2&c_seq=\"),\n",
        "    (\"ë¶€ë™ì‚°R114 - ì£¼ê°„ ì•„íŒŒíŠ¸ ì‹œí™©\", \"https://www.r114.com/news/brief?all=Y\")\n",
        "]\n",
        "\n",
        "# --- ì´ˆê¸° ì›¹ í•™ìŠµ ë° ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ë¡œì§ (ë¡œë”© í˜ì´ì§€ í‘œì‹œ) ---\n",
        "if not st.session_state[\"initial_learning_done\"]:\n",
        "    loading_container = st.container()\n",
        "    with loading_container:\n",
        "        st.write(\"\")\n",
        "        st.write(\"\")\n",
        "        st.markdown(\"<h2 style='text-align: center;'>ğŸ¡ RealView ì±—ë´‡ì´ ì§€ì‹ì„ í•™ìŠµ ì¤‘ì…ë‹ˆë‹¤...</h2>\", unsafe_allow_html=True)\n",
        "        st.markdown(\"<h4 style='text-align: center;'>ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì‹œë©´ RealViewì™€ ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</h4>\", unsafe_allow_html=True)\n",
        "        st.write(\"\")\n",
        "        progress_bar = st.progress(0, text=\"ì´ˆê¸° ì§€ì‹ ë¡œë”© ì¤€ë¹„ ì¤‘...\")\n",
        "        st.write(\"\")\n",
        "        status_text = st.empty()\n",
        "        st.write(\"\")\n",
        "\n",
        "    total_urls = len(predefined_web_urls)\n",
        "\n",
        "    for idx, (name, url) in enumerate(predefined_web_urls):\n",
        "        progress_bar.progress((idx + 1) / total_urls, text=f\"ğŸ“š ì›¹ ì§€ì‹ í•™ìŠµ ì¤‘: {name} ({(idx + 1)/total_urls:.0%})\")\n",
        "        status_text.caption(f\"âœ¨ í˜„ì¬ í•™ìŠµ ì¤‘: {name}\")\n",
        "\n",
        "        if check_url_active(url) and url not in st.session_state.web_loaded_urls:\n",
        "            try:\n",
        "                if url.lower().endswith('.pdf'):\n",
        "                    response = requests.get(url)\n",
        "                    response.raise_for_status()\n",
        "                    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
        "                        tmp_file.write(response.content)\n",
        "                        tmp_file_path = tmp_file.name\n",
        "                    loader_doc = PyPDFLoader(tmp_file_path)\n",
        "                    docs_from_web = loader_doc.load()\n",
        "                    os.remove(tmp_file_path)\n",
        "                else:\n",
        "                    loader_web = WebBaseLoader(url)\n",
        "                    docs_from_web = loader_web.load()\n",
        "\n",
        "                for doc in docs_from_web:\n",
        "                    doc.metadata['source'] = f\"{name} ({url})\"\n",
        "                all_documents.extend(docs_from_web)\n",
        "                st.session_state.web_loaded_urls.append(url)\n",
        "            except Exception as e:\n",
        "                pass\n",
        "\n",
        "    if all_documents:\n",
        "        status_text.caption(\"ğŸ§  ìµœì¢… ì§€ì‹ êµ¬ì¡°í™” ì¤‘...\")\n",
        "        st.session_state[\"vectorstore\"] = create_vectorstore(all_documents)\n",
        "        progress_bar.progress(1.0, text=\"âœ… ëª¨ë“  ì§€ì‹ í•™ìŠµ ì™„ë£Œ!\")\n",
        "        status_text.caption(\"ğŸ‰ RealView ì±—ë´‡ ì¤€ë¹„ ì™„ë£Œ!\")\n",
        "    else:\n",
        "        st.warning(\"âš ï¸ í•™ìŠµí•  ë¬¸ì„œê°€ ì—†ê±°ë‚˜ ëª¨ë“  URL ì ‘ê·¼ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ì •ë³´ë§Œìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "    loading_container.empty()\n",
        "    st.session_state[\"initial_learning_done\"] = True\n",
        "    st.rerun()\n",
        "\n",
        "\n",
        "# --- ì±—ë´‡ ëŒ€í™” ë° ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ/ê´€ë¦¬ UI (ì´ˆê¸° í•™ìŠµ ì™„ë£Œ í›„ì—ë§Œ í‘œì‹œ) ---\n",
        "if st.session_state[\"initial_learning_done\"]:\n",
        "    st.subheader(\"RealView ì±—ë´‡ì—ê²Œ ë¶€ë™ì‚° ë° ì¼ë°˜ ì§ˆë¬¸ì„ í•´ ë³´ì„¸ìš”!\")\n",
        "\n",
        "    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message.type):\n",
        "            st.markdown(message.content)\n",
        "            # âœ… ìˆ˜ì •ëœ ë¶€ë¶„: message.kwargs ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë¡œì§ ê°•í™”\n",
        "            # hasattr(message, 'kwargs')ë¡œ ë¨¼ì € kwargs ì†ì„± ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
        "            if message.type == \"ai\" and hasattr(message, 'kwargs') and \"sources\" in message.kwargs and message.kwargs[\"sources\"]:\n",
        "                with st.expander(\"ğŸ’¡ ì°¸ê³  ì¶œì²˜ ë³´ê¸°\"):\n",
        "                    st.markdown(', '.join(sorted(message.kwargs[\"sources\"])))\n",
        "\n",
        "    q = st.chat_input(\"ì˜ˆ: 2025ë…„ ë¶€ë™ì‚° ì‹œì¥ ì „ë§ì€? ë˜ëŠ” ì„œìš¸ ì•„íŒŒíŠ¸ íˆ¬ì ì‹œ ê³ ë ¤í•  ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\")\n",
        "\n",
        "    if q:\n",
        "        st.chat_message(\"user\").markdown(q)\n",
        "        st.session_state[\"messages\"].append(HumanMessage(content=q))\n",
        "\n",
        "        with st.spinner(\"RealViewê°€ ì—´ì‹¬íˆ ì •ë³´ë¥¼ ë¶„ì„ ì¤‘...\"):\n",
        "            context_docs = []\n",
        "            context_string = \"ì œê³µëœ ì°¸ê³  ë¬¸ì„œê°€ ì—†ì–´ LLMì˜ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.\"\n",
        "            sources = set()\n",
        "\n",
        "            if st.session_state[\"vectorstore\"]:\n",
        "                context_docs = st.session_state[\"vectorstore\"].similarity_search(q, k=5)\n",
        "                context_string = \"\\n\\n\".join(d.page_content for d in context_docs)\n",
        "\n",
        "                for d in context_docs:\n",
        "                    if 'source' in d.metadata:\n",
        "                        sources.add(d.metadata['source'])\n",
        "\n",
        "            history_for_llm = []\n",
        "            for msg in st.session_state[\"messages\"][:-1]:\n",
        "                if isinstance(msg, HumanMessage):\n",
        "                    history_for_llm.append(HumanMessage(content=msg.content))\n",
        "                elif isinstance(msg, AIMessage):\n",
        "                    # âœ… ìˆ˜ì •ëœ ë¶€ë¶„: getattr(msg, 'kwargs', {})ë¥¼ ì‚¬ìš©í•˜ì—¬ kwargsê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ë”•ì…”ë„ˆë¦¬ ì „ë‹¬\n",
        "                    history_for_llm.append(AIMessage(content=msg.content, kwargs=getattr(msg, 'kwargs', {})))\n",
        "\n",
        "            raw_answer = chain.invoke({\n",
        "                'context': context_string,\n",
        "                'history': history_for_llm,\n",
        "                'q': q\n",
        "            })\n",
        "\n",
        "            relevant_sources = set()\n",
        "            for doc in context_docs:\n",
        "                source_url = doc.metadata.get('source')\n",
        "                if source_url and raw_answer.lower() in doc.page_content.lower():\n",
        "                    relevant_sources.add(source_url)\n",
        "\n",
        "            if \"ì°¸ê³  ë¬¸ì„œê°€ ì—†ì–´\" in raw_answer.lower() or \"ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤\" in raw_answer.lower():\n",
        "                relevant_sources.clear()\n",
        "\n",
        "            if relevant_sources:\n",
        "                ai_message_object = AIMessage(content=raw_answer, kwargs={\"sources\": sorted(list(relevant_sources))})\n",
        "            else:\n",
        "                ai_message_object = AIMessage(content=raw_answer)\n",
        "\n",
        "        st.chat_message(\"assistant\").markdown(raw_answer)\n",
        "        if relevant_sources:\n",
        "            with st.expander(\"ğŸ’¡ ì°¸ê³  ì¶œì²˜ ë³´ê¸°\"):\n",
        "                st.markdown(', '.join(sorted(list(relevant_sources))))\n",
        "        st.session_state[\"messages\"].append(ai_message_object)\n",
        "\n",
        "\n",
        "    with st.expander(\"ğŸ› ï¸ í•™ìŠµ ë° ê´€ë¦¬ (ê´€ë¦¬ììš©)\"):\n",
        "        st.markdown(\"ì—¬ê¸°ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ìƒˆë¡œìš´ ì›¹ í˜ì´ì§€ URLì„ í•™ìŠµì‹œì¼œ RealView ì±—ë´‡ì˜ ì§€ì‹ì„ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "        uploaded_files = st.file_uploader(\n",
        "            \"1. ìƒˆë¡œìš´ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì‹­ì‹œì˜¤ (ì—¬ëŸ¬ íŒŒì¼ ê°€ëŠ¥)\",\n",
        "            type=\"pdf\",\n",
        "            accept_multiple_files=True,\n",
        "            key=\"pdf_uploader\"\n",
        "        )\n",
        "\n",
        "        if uploaded_files:\n",
        "            new_docs_from_pdf = []\n",
        "            for uploaded_file in uploaded_files:\n",
        "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
        "                    tmp_file.write(uploaded_file.read())\n",
        "                    tmp_file_path = tmp_file.name\n",
        "\n",
        "                st.info(f\"âœ¨ PDF íŒŒì¼ '{uploaded_file.name}' ì—…ë¡œë“œ ì™„ë£Œ!\")\n",
        "                loader_pdf = PyPDFLoader(tmp_file_path)\n",
        "                docs_from_pdf = loader_pdf.load()\n",
        "                for doc in docs_from_pdf:\n",
        "                    doc.metadata['source'] = uploaded_file.name\n",
        "                new_docs_from_pdf.extend(docs_from_pdf)\n",
        "                os.remove(tmp_file_path)\n",
        "\n",
        "            if new_docs_from_pdf:\n",
        "                st.session_state[\"vectorstore\"] = create_vectorstore(all_documents + new_docs_from_pdf)\n",
        "                all_documents.extend(new_docs_from_pdf)\n",
        "                st.success(\"âœ… ìƒˆë¡œìš´ PDF ë¬¸ì„œ í•™ìŠµ ë° ë²¡í„° ì €ì¥ì†Œ ì—…ë°ì´íŠ¸ ì™„ë£Œ!\")\n",
        "                st.rerun()\n",
        "\n",
        "\n",
        "        url_input_manual = st.text_input(\"2. ìƒˆë¡œìš´ ì›¹ í˜ì´ì§€ URLì„ ì…ë ¥í•˜ì‹­ì‹œì˜¤ (ì˜ˆ: https://example.com/article)\", key=\"manual_url_input\")\n",
        "\n",
        "        if url_input_manual and url_input_manual not in st.session_state.web_loaded_urls:\n",
        "            if check_url_active(url_input_manual):\n",
        "                try:\n",
        "                    with st.spinner(f\"ìƒˆë¡œìš´ ì›¹ í˜ì´ì§€ '{url_input_manual}' ë‚´ìš©ì„ í•™ìŠµ ì¤‘... ğŸŒ\"):\n",
        "                        if url_input_manual.lower().endswith('.pdf'):\n",
        "                            response = requests.get(url_input_manual)\n",
        "                            response.raise_for_status()\n",
        "                            with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
        "                                tmp_file.write(response.content)\n",
        "                                tmp_file_path = tmp_file.name\n",
        "                            loader_doc = PyPDFLoader(tmp_file_path)\n",
        "                            docs_from_web = loader_doc.load()\n",
        "                            os.remove(tmp_file_path)\n",
        "                        else:\n",
        "                            loader_web = WebBaseLoader(url_input_manual)\n",
        "                            docs_from_web = loader_web.load()\n",
        "\n",
        "                        for doc in docs_from_web:\n",
        "                            doc.metadata['source'] = url_input_manual\n",
        "\n",
        "                        st.session_state[\"vectorstore\"] = create_vectorstore(all_documents + docs_from_web)\n",
        "                        all_documents.extend(docs_from_web)\n",
        "                        st.session_state.web_loaded_urls.append(url_input_manual)\n",
        "                        st.success(f\"ğŸ‰ ì›¹ í˜ì´ì§€ '{url_input_manual}' í•™ìŠµ ì™„ë£Œ ë° ë²¡í„° ì €ì¥ì†Œ ì—…ë°ì´íŠ¸ ì™„ë£Œ!\")\n",
        "                        st.rerun()\n",
        "                except Exception as e:\n",
        "                    st.error(f\"âŒ ì›¹ í˜ì´ì§€ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. URLì„ í™•ì¸í•˜ê±°ë‚˜, íŠ¹ì • ê¸°ì‚¬/ë¸”ë¡œê·¸ ë§í¬ë¥¼ ì‹œë„í•´ ë³´ì‹­ì‹œì˜¤!\")\n",
        "                    st.info(\"ğŸ’¡ ë„¤ì´ë²„ ë¶€ë™ì‚° ë‰´ìŠ¤ ë©”ì¸ í˜ì´ì§€ì™€ ê°™ì´ ë³µì¡í•œ êµ¬ì„±ì˜ í˜ì´ì§€ëŠ” ë‚´ìš© ì¶”ì¶œì´ ì–´ë µê±°ë‚˜ ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¸”ë¡œê·¸ ê¸€ì´ë‚˜ íŠ¹ì • ë‰´ìŠ¤ ê¸°ì‚¬ URLì„ ì…ë ¥í•˜ë©´ ë” ì¢‹ìŠµë‹ˆë‹¤.\")\n",
        "            else:\n",
        "                st.error(f\"âŒ ì…ë ¥ëœ URLì´ í™œì„±í™”ë˜ì§€ ì•Šê±°ë‚˜ ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url_input_manual}\")\n",
        "        elif url_input_manual in st.session_state.web_loaded_urls:\n",
        "            st.info(f\"âœ… ì›¹ í˜ì´ì§€ '{url_input_manual}'ëŠ” ì´ë¯¸ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJYGgVDNmrzs",
        "outputId": "ef99537a-7396-4652-9296-18787fd38d38"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import socket\n",
        "from pyngrok import ngrok\n",
        "from pyngrok.exception import PyngrokNgrokHTTPError, PyngrokNgrokURLError\n",
        "\n",
        "# ì´ì „ ngrok í„°ë„ ì¢…ë£Œ (ì•ˆì „í•˜ê²Œ ì‹œì‘í•˜ê¸° ìœ„í•´)\n",
        "try:\n",
        "    print(\"ê¸°ì¡´ ngrok í„°ë„ì„ ì¢…ë£Œ ì‹œë„í•©ë‹ˆë‹¤...\")\n",
        "    ngrok.kill()\n",
        "    print(\"ê¸°ì¡´ ngrok í„°ë„ ì¢…ë£Œ ì™„ë£Œ.\")\n",
        "except Exception as e:\n",
        "    print(f\"ê¸°ì¡´ ngrok í„°ë„ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì´ë¯¸ ì¢…ë£Œë˜ì—ˆì„ ìˆ˜ ìˆìŒ ë˜ëŠ” ì‹¤í–‰ ì¤‘ì¸ ngrok ì—†ìŒ): {e}\")\n",
        "\n",
        "# Streamlit ì•± ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰\n",
        "print(\"Streamlit ì•±ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤...\")\n",
        "os.system(\"streamlit run app.py --server.address 0.0.0.0 --server.port 8501 &>/dev/null &\")\n",
        "\n",
        "# Streamlit ì•±ì´ 8501 í¬íŠ¸ì—ì„œ ì‘ë‹µí•  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ëŠ” í•¨ìˆ˜\n",
        "def wait_for_streamlit_app(port, timeout=90):\n",
        "    start_time = time.time()\n",
        "    while True:\n",
        "        if time.time() - start_time > timeout:\n",
        "            print(f\"âŒ {timeout}ì´ˆ ë™ì•ˆ Streamlit ì•±ì´ {port}ë²ˆ í¬íŠ¸ì—ì„œ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "            return False\n",
        "        try:\n",
        "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "                s.settimeout(1)\n",
        "                s.connect((\"127.0.0.1\", port))\n",
        "            print(f\"âœ… Streamlit ì•±ì´ {port}ë²ˆ í¬íŠ¸ì—ì„œ ì‘ë‹µì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤!\")\n",
        "            return True\n",
        "        except (socket.error, ConnectionRefusedError):\n",
        "            print(f\"â³ Streamlit ì•± ëŒ€ê¸° ì¤‘... ({int(time.time() - start_time)}ì´ˆ ê²½ê³¼ / ìµœëŒ€ {timeout}ì´ˆ)\")\n",
        "            time.sleep(2)\n",
        "\n",
        "# Streamlit ì•±ì´ ì™„ì „íˆ ë¡œë“œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.\n",
        "if not wait_for_streamlit_app(8501, timeout=90):\n",
        "    print(\"âŒ Streamlit ì•± ì‹œì‘ì— ì‹¤íŒ¨í•˜ì—¬ ngrok ì—°ê²°ì„ ì‹œë„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
        "else:\n",
        "    # ngrok í„°ë„ ì—´ê¸°\n",
        "    print(\"ngrok í„°ë„ì„ ì—°ê²°í•©ë‹ˆë‹¤...\")\n",
        "    try:\n",
        "        public_url = ngrok.connect(addr=\"http://127.0.0.1:8501\", bind_tls=True)\n",
        "        print(\"ğŸŒ Streamlit ì•±ì´ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤! ì•„ë˜ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”:\")\n",
        "        print(public_url)\n",
        "    except (PyngrokNgrokHTTPError, PyngrokNgrokURLError) as e:\n",
        "        print(f\"âŒ ngrok í„°ë„ ì—°ê²° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {type(e).__name__} - {e}\")\n",
        "        print(\"pyngrok ë˜ëŠ” ngrok ì„œë¹„ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
        "        print(\"1. 'ngrok config add-authtoken YOUR_AUTH_TOKEN' ëª…ë ¹ì–´ë¡œ ì¸ì¦ í† í°ì„ ë‹¤ì‹œ ì„¤ì •í•´ë³´ì„¸ìš”. (ë°˜ë“œì‹œ í°ë”°ì˜´í‘œ ì•ˆì— í† í°ì„ ë„£ì–´ì£¼ì„¸ìš”!)\")\n",
        "        print(\"2. ngrok ìœ ë£Œ í”Œëœì´ ì•„ë‹Œ ê²½ìš°, ì—°ê²° ìš”ì²­ì´ ë„ˆë¬´ ë§ì•„ì„œ ì¼ì‹œì ìœ¼ë¡œ ì°¨ë‹¨ë˜ì—ˆì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ë³´ì„¸ìš”.\")\n",
        "        print(\"3. ì½”ë© ì„¸ì…˜ ì´ˆê¸°í™” í›„ ëª¨ë“  ì…€ì„ ìˆœì„œëŒ€ë¡œ ë‹¤ì‹œ ì‹¤í–‰í•´ ë³´ì„¸ìš”.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {type(e).__name__} - {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQHM9ZwemxIx",
        "outputId": "fcd27e59-c685-4356-bf5b-47bab07d7f53"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ê¸°ì¡´ ngrok í„°ë„ì„ ì¢…ë£Œ ì‹œë„í•©ë‹ˆë‹¤...\n",
            "ê¸°ì¡´ ngrok í„°ë„ ì¢…ë£Œ ì™„ë£Œ.\n",
            "Streamlit ì•±ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤...\n",
            "â³ Streamlit ì•± ëŒ€ê¸° ì¤‘... (0ì´ˆ ê²½ê³¼ / ìµœëŒ€ 90ì´ˆ)\n",
            "â³ Streamlit ì•± ëŒ€ê¸° ì¤‘... (2ì´ˆ ê²½ê³¼ / ìµœëŒ€ 90ì´ˆ)\n",
            "â³ Streamlit ì•± ëŒ€ê¸° ì¤‘... (4ì´ˆ ê²½ê³¼ / ìµœëŒ€ 90ì´ˆ)\n",
            "âœ… Streamlit ì•±ì´ 8501ë²ˆ í¬íŠ¸ì—ì„œ ì‘ë‹µì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤!\n",
            "ngrok í„°ë„ì„ ì—°ê²°í•©ë‹ˆë‹¤...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2025-11-12T05:48:00+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: The authtoken you specified does not look like a proper ngrok authtoken.\\nYour authtoken: YOUR_AUTHTOKEN_HERE\\nInstructions to install your authtoken are on your ngrok dashboard:\\nhttps://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_105\\r\\n\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: PyngrokNgrokError - The ngrok process errored on start: authentication failed: The authtoken you specified does not look like a proper ngrok authtoken.\\nYour authtoken: YOUR_AUTHTOKEN_HERE\\nInstructions to install your authtoken are on your ngrok dashboard:\\nhttps://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_105\\r\\n.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tinGv2xGmzQQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}